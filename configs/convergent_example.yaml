seed: 42
device: "cuda"

task:
  name: "cv"            # "cv" or "nlp"
  dataset: "CIFAR10"    # CV demo
  num_classes: 10

federated:
  rounds: 50            # More rounds for convergence
  num_clients: 10
  clients_per_round: 5
  local_epochs: 5       # More local training
  batch_size: 64
  lr: 0.1               # Higher learning rate to compensate for compression
  optimizer: "sgd"      # "sgd" or "adam"
  momentum: 0.9

noniid:
  enabled: true
  method: "dirichlet"
  alpha: 1.0            # Less non-IID (higher alpha = more IID)
  min_size: 50

model:
  family: "cnn"        # "cnn" | "vit" | "llm"
  name: "resnet18"     # cnn: resnet18, mobilenet_v3_small...
  vit_name: "vit_tiny_patch16_224"  # if family=vit (requires timm)
  llm_name: "gpt2"     # if family=llm (requires transformers)
  trainable_scope: "full"   # "full" | "head" | "lora_stub"

compression:
  enabled: true
  method: "topk_ef"    # "none" | "topk" | "topk_ef" | "quant8" | "quant8_ef"
  topk_ratio: 0.1      # 10% elements kept (less aggressive)
  quant_bits: 8
  ef: true

bits:
  count_downlink: true
  count_uplink: true